{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50. データの入手・整形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．  \n",
    "\n",
    "ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．  \n",
    "情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．  \n",
    "抽出された事例をランダムに並び替える．  \n",
    "抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．  \n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを読み込む\n",
    "df = pd.read_csv('newsCorpora.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "#データを抽出する\n",
    "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを分割する\n",
    "#train : valid,test = 8:2\n",
    "train, valid_test = train_test_split(df, test_size = 0.2, shuffle=True, random_state=0, stratify=df['CATEGORY'])\n",
    "#valid,test を半分に分ける　8:1:1にする\n",
    "valid, test = train_test_split(valid_test, test_size = 0.5, shuffle=True, random_state=0, stratify=valid_test['CATEGORY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを各ファイルに保存する\n",
    "train.to_csv('./train.txt', sep='\\t', index=False)\n",
    "valid.to_csv('./valid.txt', sep='\\t', index=False)\n",
    "test.to_csv('./test.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt\n",
      "b    4502\n",
      "e    4223\n",
      "t    1219\n",
      "m     728\n",
      "Name: CATEGORY, dtype: int64\n",
      "valid.txt\n",
      "b    562\n",
      "e    528\n",
      "t    153\n",
      "m     91\n",
      "Name: CATEGORY, dtype: int64\n",
      "test.txt\n",
      "b    563\n",
      "e    528\n",
      "t    152\n",
      "m     91\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#各カテゴリの事例数を確認\n",
    "print('train.txt')\n",
    "print(train['CATEGORY'].value_counts())\n",
    "print('valid.txt')\n",
    "print(valid['CATEGORY'].value_counts())\n",
    "print('test.txt')\n",
    "print(test['CATEGORY'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51. 特徴量抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前処理\n",
    "import string\n",
    "import re\n",
    "\n",
    "def preprocessing(text):\n",
    "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(table)  # 記号をスペースに置換\n",
    "    text = text.lower()  # 小文字化\n",
    "    text = re.sub('[0-9]+', '0', text)  # 数字列を0に置換\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの結合\n",
    "df = pd.concat([train, valid, test], axis=0)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "df['TITLE'] = df['TITLE'].map(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# データの分割\n",
    "train_valid = df[:len(train) + len(valid)]\n",
    "test = df[len(train) + len(valid):]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10)　#出現数が少ない単語は省く\n",
    "\n",
    "X_train_valid = vectorizer.fit_transform(train_valid['TITLE'])  \n",
    "X_test = vectorizer.transform(test['TITLE']) #testは分けておく\n",
    "\n",
    "X_train_valid = pd.DataFrame(X_train_valid.toarray(), columns=vectorizer.get_feature_names())\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# trainとvalidを分割\n",
    "X_train = X_train_valid[:len(train)]\n",
    "X_valid = X_train_valid[len(train):]\n",
    "\n",
    "# データの保存\n",
    "X_train.to_csv('train.feature.txt', sep='\\t', index=False)\n",
    "X_valid.to_csv('valid.feature.txt', sep='\\t', index=False)\n",
    "X_test.to_csv('test.feature.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0d   0m  0million  0nd   0s  0st  0th   aa  aaliyah  abbvie  ...  yields  \\\n",
      "0  0.0  0.0       0.0  0.0  0.0  0.0  0.0  0.0      0.0     0.0  ...     0.0   \n",
      "1  0.0  0.0       0.0  0.0  0.0  0.0  0.0  0.0      0.0     0.0  ...     0.0   \n",
      "2  0.0  0.0       0.0  0.0  0.0  0.0  0.0  0.0      0.0     0.0  ...     0.0   \n",
      "3  0.0  0.0       0.0  0.0  0.0  0.0  0.0  0.0      0.0     0.0  ...     0.0   \n",
      "4  0.0  0.0       0.0  0.0  0.0  0.0  0.0  0.0      0.0     0.0  ...     0.0   \n",
      "\n",
      "   york  you  young  your   yr  yuan  zac  zendaya  zone  \n",
      "0   0.0  0.0    0.0   0.0  0.0   0.0  0.0      0.0   0.0  \n",
      "1   0.0  0.0    0.0   0.0  0.0   0.0  0.0      0.0   0.0  \n",
      "2   0.0  0.0    0.0   0.0  0.0   0.0  0.0      0.0   0.0  \n",
      "3   0.0  0.0    0.0   0.0  0.0   0.0  0.0      0.0   0.0  \n",
      "4   0.0  0.0    0.0   0.0  0.0   0.0  0.0      0.0   0.0  \n",
      "\n",
      "[5 rows x 2139 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 52. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=0,max_iter=1000)\n",
    "model.fit(X_train, train['CATEGORY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 53. 予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_score(model, x):\n",
    "    return [np.max(model.predict_proba(x), axis=1), model.predict(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.38143102, 0.9162524 , 0.71503472, ..., 0.86423888, 0.97127036,\n",
      "       0.73518722]), array(['b', 'b', 'e', ..., 'b', 'b', 'e'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_score(model, X_train)\n",
    "test_pred = predict_with_score(model, X_test)\n",
    "\n",
    "print(train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 54. 正解率の計測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データの正解率：0.9258808095952024\n",
      "評価データの正解率：0.8755622188905547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_accuracy = accuracy_score(train['CATEGORY'], train_pred[1])\n",
    "test_accuracy = accuracy_score(test['CATEGORY'], test_pred[1])\n",
    "\n",
    "print('学習データの正解率：' + str(train_accuracy))\n",
    "print('評価データの正解率：' + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 55. 混同行列の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4341   96    9   56]\n",
      " [  55 4155    2   11]\n",
      " [  86  130  498   14]\n",
      " [ 183  141    8  887]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_cm = confusion_matrix(train['CATEGORY'], train_pred[1])\n",
    "print(train_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
